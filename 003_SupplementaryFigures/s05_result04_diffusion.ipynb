{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8742a8-65b2-4ee9-824d-d550cbfb4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import umap\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from PIL import Image,ImageOps\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d33b3-0d9d-438b-b161-19b52d5b6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/jinny/projects/Art-history/Art-history/datas/'\n",
    "file_info = pd.read_csv(base_path+'file_info.csv')\n",
    "\n",
    "# file_info_latent\n",
    "df = pd.DataFrame(( np.load(base_path+'vectors/avec_latents.npy', allow_pickle=True)),columns=['avec','Path'])\n",
    "file_info_latents = pd.merge(file_info, df, how = 'left', on = 'Path')\n",
    "file_info_latents = file_info_latents[~file_info_latents.avec.isnull()]\n",
    "\n",
    "avec = np.array([i.reshape(-1) for i in file_info_latents['avec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f5a5b-f46c-4947-a7ec-92a4a8170a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load(base_path+'vectors/cvec_latents.npy', allow_pickle=True),columns=['cvec','Path'])\n",
    "file_info_latents = pd.merge(file_info_latents, df, how = 'left', on = 'Path')\n",
    "\n",
    "cvec = np.array([i.reshape(-1) for i in file_info_latents['cvec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bffcc56-45ae-42f8-88f8-310b58c41eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'words/tokens.pkl', 'rb') as file:\n",
    "    tokens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02b9ee-a107-4931-a8b7-4f25e3f8845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'vectors/diffusion/diffusion_sample.pkl', 'rb') as file:\n",
    "    painting = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b71518-69bb-4bbc-94dc-afc94b87bbc7",
   "metadata": {},
   "source": [
    "# make white noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85afd2-e329-40db-bd07-801dceec639f",
   "metadata": {},
   "source": [
    "### using history kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e307f9-a671-4aa7-bb47-91721a761625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1d6f2-bdc9-4da9-b461-3a71546ce721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "from imwatermark import WatermarkEncoder\n",
    "import pandas as pd\n",
    "from scripts.txt2img import put_watermark\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde21c3-7836-4afb-a930-2d7a9c3f99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model(cudanum) :\n",
    "    base_path = '/home/jinny/projects/Art-history/Art-history/'\n",
    "    config = OmegaConf.load(base_path+f\"scripts/configs/stable-diffusion/v2-inference.yaml\")\n",
    "    model = load_model_from_config(config, base_path+f\"scripts/configs/checkpoint/512-base-ema.ckpt\") \n",
    "\n",
    "    GPU_NUM=cudanum\n",
    "    \n",
    "    # GPU 할당 변경하기\n",
    "    device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(device) # change allocation of current GPU\n",
    "    print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model,device\n",
    "\n",
    "def load_img(path):\n",
    "    image = path.convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f2b50-8344-475e-b2fb-8cebc7c51585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2img(latent_samples, model) :\n",
    "    precision_scope = autocast \n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                \n",
    "                x_samples = model.decode_first_stage(latent_samples)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd52791-43cf-41d9-8360-cf2efa06a426",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model load\n",
    "model,device = load_model(3)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "863130cb-0eb9-427a-939a-7a86c5c03b78",
   "metadata": {},
   "source": [
    "white_noise_img = [Image.fromarray((np.random.rand(512, 512, 3)*255).astype(np.uint8)) for i in range(100)]\n",
    "\n",
    "with open(base_path+'vectors/white_noise_img.pkl', 'wb') as file:\n",
    "    pickle.dump(white_noise_img, file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6415801c-a4c5-4516-acb9-d663f5643bef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "white_noise_avec = list()\n",
    "for i in white_noise_img :\n",
    "    init_image = load_img(i).to(device)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "    white_noise_avec.append(init_latent.view([-1]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0730bb0-dae4-4b77-9d18-c1111bdae26e",
   "metadata": {},
   "source": [
    "with open(base_path+'vectors/white_noise_avec.pkl', 'wb') as file:\n",
    "    pickle.dump([np.array(i.reshape(-1).cpu()) for i in white_noise_avec], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd749ee-599a-43bc-901b-dcafc05ef8d8",
   "metadata": {},
   "source": [
    "### using clip_inter kernel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d554248e-55aa-43ae-af81-8a8f20a7bf4e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator\n",
    "\n",
    "import clip_interrogator\n",
    "clip_interrogator.__file__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdeb733f-520d-4932-83ab-c7d7a2adcd0b",
   "metadata": {},
   "source": [
    "base_path = '/home/jinny/projects/Art-history/Art-history/datas/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "274501b3-3499-4b2a-9896-0b6b2873bbf9",
   "metadata": {},
   "source": [
    "with open(base_path+'vectors/white_noise_img.pkl', 'rb') as file:\n",
    "    img = pickle.load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af7aa392-31a3-4db1-a3d4-2467c63cc049",
   "metadata": {},
   "source": [
    "device = torch.device(f'cuda:{3}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "ci = Interrogator(Config(clip_model_name=\"ViT-H-14/laion2b_s32b_b79k\",device=device))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fb6b1bd-68f0-4c3e-897d-585d403d9545",
   "metadata": {},
   "source": [
    "white_noise_cvec = [ ci.image_to_features(i).cpu() for i in img ]\n",
    "\n",
    "with open(base_path+'vectors/white_noise_cvec.pkl', 'wb') as file:\n",
    "    pickle.dump(white_noise_cvec, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb6994-6a73-4043-8e1e-1ec9b1790e31",
   "metadata": {},
   "source": [
    "# regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfb413-ec79-452b-9446-d34d93d17c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec1f32-58f5-411e-9da3-df4779a748d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'vectors/white_noise_cvec.pkl', 'rb') as file:\n",
    "    white_noise_cvec = pickle.load(file)\n",
    "\n",
    "with open(base_path+'vectors/white_noise_avec.pkl', 'rb') as file:\n",
    "    white_noise_avec = pickle.load(file)\n",
    "\n",
    "with open(base_path+'vectors/white_noise_img.pkl', 'rb') as file:\n",
    "    white_noise_img = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a406fc-8d3f-45cc-a022-cbad1d7d3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "cvec_xgb_model = xgb.XGBRegressor()\n",
    "cvec_xgb_model.load_model(base_path+\"models/cvec_xgb_model.json\")\n",
    "\n",
    "avec_xgb_model = xgb.XGBRegressor()\n",
    "avec_xgb_model.load_model(base_path+\"models/avec_xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb62556-4746-44a6-8581-ef9e2698f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_white = [cvec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1).cpu()) for i in white_noise_cvec])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b4275-5acb-4047-a5cb-02e09365caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "plt.imshow(white_noise_img[0])\n",
    "plt.axis('off')\n",
    "plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/supple/si11_img.svg')\n",
    "print(y_pre_white[0][0]*10+1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800b3d9-6db3-4857-9efe-e0f26ae4bc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "blue_l = '#0054FF'\n",
    "\n",
    "sns.distplot(y_pre_white,color=blue_l, hist=False)\n",
    "\n",
    "ax.set_xlabel('Predict Year', fontsize=32)\n",
    "ax.set_ylabel('Density', fontsize=32)\n",
    "ax.set_xticks([0, 10, 20, 30, 40,50], [1500, 1600, 1700, 1800, 1900,2000], fontsize=26)\n",
    "ax.set_yticks([0,0.3,0.6,0.9,1.2], [0,0.3,0.6,0.9,1.2], fontsize=26)\n",
    "ax.set_title('C-vector', fontsize=38)\n",
    "\n",
    "plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/supple/si11_c.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f0f22-0c71-40b1-9dbc-ee0bc834bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre_white = [avec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1)) for i in white_noise_avec])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd991669-bd08-461d-bdc8-346b57be2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "blue_l = '#0054FF'\n",
    "\n",
    "sns.distplot(y_pre_white,color=blue_l, hist=False)\n",
    "\n",
    "ax.set_xlabel('Predict Year', fontsize=32)\n",
    "ax.set_ylabel('Density', fontsize=32)\n",
    "ax.set_xticks([0, 10, 20, 30, 40,50], [1500, 1600, 1700, 1800, 1900,2000], fontsize=26)\n",
    "ax.set_yticks([0,0.1,0.2,0.3,0.4], [0,0.1,0.2,0.3,0.4], fontsize=26)\n",
    "ax.set_title('A-vector', fontsize=38)\n",
    "\n",
    "plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/supple/si11_a.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8386e-54e6-4ba7-93cf-56dae03abd3c",
   "metadata": {},
   "source": [
    "# Diffusion include comma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e863b55-cebf-4251-b2ad-21dda298f436",
   "metadata": {},
   "source": [
    "#### vector import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec3860-68e0-43ad-bd71-28b33a39b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import umap\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from PIL import Image,ImageOps\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540df51-f91d-409f-b533-1ec981da423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/jinny/projects/Art-history/Art-history/datas/'\n",
    "file_info = pd.read_csv(base_path+'file_info.csv')\n",
    "\n",
    "# file_info_latent\n",
    "df = pd.DataFrame(( np.load(base_path+'vectors/avec_latents.npy', allow_pickle=True)),columns=['avec','Path'])\n",
    "file_info_latents = pd.merge(file_info, df, how = 'left', on = 'Path')\n",
    "file_info_latents = file_info_latents[~file_info_latents.avec.isnull()]\n",
    "\n",
    "avec = np.array([i.reshape(-1) for i in file_info_latents['avec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d937665-7efd-4566-a271-20fd09621af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load(base_path+'vectors/cvec_latents.npy', allow_pickle=True),columns=['cvec','Path'])\n",
    "file_info_latents = pd.merge(file_info_latents, df, how = 'left', on = 'Path')\n",
    "\n",
    "cvec = np.array([i.reshape(-1) for i in file_info_latents['cvec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017f894-7625-4f49-b39e-9dc8d911a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'words/tokens.pkl', 'rb') as file:\n",
    "    tokens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108e6404-c7d8-44b7-8761-f232c625851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'vectors/diffusion/diffusion_sample.pkl', 'rb') as file:\n",
    "    painting = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72ce16f-f3c5-4de0-afc2-05a2365a9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_prompt = defaultdict(dict)\n",
    "using_noprompt = defaultdict(dict)\n",
    "\n",
    "for vec in ['avec','cvec','img'] :\n",
    "    vec_path = f'vectors/diffusion/comma/{vec}/'\n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        with open(base_path + vec_path + step+'/using_prompt.pkl', 'rb') as file:\n",
    "            using_prompt[vec][step] = pickle.load(file)\n",
    "\n",
    "for vec in ['avec','cvec','img'] :\n",
    "    vec_path = f'vectors/diffusion/comma/{vec}/'\n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        with open(base_path + vec_path + step+'/using_noprompt.pkl', 'rb') as file:\n",
    "            using_noprompt[vec][step] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f250d5-f8a2-4554-b092-883fa3aca758",
   "metadata": {},
   "source": [
    "#### diffusion import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12521a-e78f-40b8-b82d-d938e99e9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model(device_num) :\n",
    "    base_path = '/home/jinny/projects/Art-history/Art-history/'\n",
    "    config = OmegaConf.load(base_path+f\"scripts/configs/stable-diffusion/v2-inference.yaml\")\n",
    "    model = load_model_from_config(config, base_path+f\"scripts/configs/checkpoint/512-base-ema.ckpt\") \n",
    "\n",
    "    GPU_NUM = device_num # 원하는 GPU 번호 입력\n",
    "    \n",
    "    # GPU 할당 변경하기\n",
    "    device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(device) # change allocation of current GPU\n",
    "    print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model,device\n",
    "\n",
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdd031-2a20-4d1a-bb7b-8d4efa24144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2latent(path,model) :\n",
    "    file_info = pd.read_csv(base_path+'file_info.csv')\n",
    "\n",
    "    image_latent = list()\n",
    "    for i in path :\n",
    "        init_image = load_img(base_path+'resized_image/'+i).to(device)\n",
    "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "        init_latent = init_latent.view([-1])\n",
    "        image_latent.append(init_latent.cpu().numpy())\n",
    "\n",
    "    #latent_df = pd.DataFrame(image_latent)\n",
    "    return image_latent\n",
    "\n",
    "def model_2img(latent_samples, model) :\n",
    "    precision_scope = autocast \n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                \n",
    "                x_samples = model.decode_first_stage(latent_samples)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                    #img = put_watermark(img, wm_encoder)\n",
    "                    return img\n",
    "                    # plt.imshow(img)\n",
    "                    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522d8b7-b209-4a04-805b-c8b170cccb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_model_changing_denoising_steps(init_latent,prompt,n,num,batch,model,device,step):  \n",
    "\n",
    "    seed_everything(42)\n",
    "    \n",
    "    ddim_steps = 50\n",
    "    ddim_eta = 0.0\n",
    "    n_iter = 1\n",
    "    batch_size = batch\n",
    "    scale = 9.0\n",
    "    \n",
    "    sampler = DDIMSampler(model)\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "    precision_scope = autocast\n",
    "    \n",
    "    all_samples = []\n",
    "    all_samples_enc = []\n",
    "    for i in range(n):\n",
    "        #strength = 0.8\n",
    "        strength = 1/ddim_steps * (i+num) # change strength to control denoising steps\n",
    "        data = [batch_size * [prompt]]\n",
    "        t_enc = step\n",
    "        sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                        for prompts in tqdm(data, desc=\"data\"):\n",
    "                            uc = None\n",
    "                            if scale != 1.0:\n",
    "                                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                            if isinstance(prompts, tuple):\n",
    "                                prompts = list(prompts)\n",
    "                            c = model.get_learned_conditioning(prompts)\n",
    "                            if prompt == \"no prompt\" :\n",
    "                                print(\"change prompt vector to zero\")\n",
    "                                c = torch.zeros(c.shape[0], c.shape[1], c.shape[2]).to('cuda:1') # change prompt vector to zero (i.e., no direction)\n",
    "\n",
    "                            # encode (scaled latent)\n",
    "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc] * batch_size).to(device))\n",
    "                            # decode it\n",
    "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=uc, )\n",
    "\n",
    "                            x_samples = model.decode_first_stage(samples)\n",
    "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                            \n",
    "        #draw_image(x_samples[0])\n",
    "        all_samples.append(x_samples)\n",
    "        all_samples_enc.append(samples)\n",
    "               \n",
    "    return all_samples_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c8400-f48c-4552-8590-0203ffcae744",
   "metadata": {},
   "source": [
    "# regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724099f9-375d-4535-9d42-0acd4f1abedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af431b6-cabc-4e37-a0a9-5629e846293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "cvec_xgb_model = xgb.XGBRegressor()\n",
    "cvec_xgb_model.load_model(base_path+\"models/cvec_xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62147c19-9837-4a85-8c37-ce7ef9d35798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ori = list()\n",
    "for year in [1500, 1600, 1700, 1800] :\n",
    "    y_ori.extend([int((file_info_latents['new_date'][file_info_latents['Path']==i[1]].values[0]%1500)/10) for i in painting[year]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e513315-b406-4600-82d6-a3565b09a7d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pre_diff = dict()\n",
    "\n",
    "y_pre_diff[5] = defaultdict(list)\n",
    "y_pre_diff[10] = defaultdict(list)\n",
    "y_pre_diff[20] = defaultdict(list)\n",
    "y_pre_diff[30] = defaultdict(list)\n",
    "\n",
    "for year in [1500, 1600, 1700, 1800] :\n",
    "    for step in [5,10,20,30] :\n",
    "        y_pre_diff[step]['u_prompt'].extend(cvec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1).cpu()) for i in using_prompt['cvec'][f'step{step}'][year]]))))\n",
    "        y_pre_diff[step]['u_noprompt'].extend(cvec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1).cpu()) for i in using_noprompt['cvec'][f'step{step}'][year]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8361a8c-fc5f-4a5a-a1eb-5ac48fb76229",
   "metadata": {},
   "source": [
    "### graph-abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fedf5d-1909-4ebd-9268-ab0177aa6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25, 5))\n",
    "\n",
    "blue_l = '#0054FF'\n",
    "blue_s = '#78ACF6'\n",
    "red_l = '#FF4848'\n",
    "red_s = '#FFA2A2'\n",
    "\n",
    "for idx, step in enumerate([5, 10, 20, 30]):\n",
    "    sns.regplot(\n",
    "        x=y_ori,\n",
    "        y=y_pre_diff[step]['u_noprompt'],\n",
    "        scatter_kws={'s': 10, 'alpha': 0.3, 'color': blue_s},\n",
    "        line_kws={'color': blue_l},\n",
    "        ax=ax[idx],\n",
    "        lowess=True,\n",
    "    )\n",
    "    sns.regplot(\n",
    "        x=y_ori,\n",
    "        y=y_pre_diff[step]['u_prompt'],\n",
    "        scatter_kws={'s': 10, 'alpha': 0.3, 'color': red_s},\n",
    "        line_kws={'color': red_l},\n",
    "        ax=ax[idx],\n",
    "        lowess=True,\n",
    "    )\n",
    "\n",
    "# 범례 라인 생성\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=blue_l, lw=2, label='Random diffusion'),\n",
    "    Line2D([0], [0], color=red_l, lw=2, label='Future directed'),\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].plot([-5, 55], [-5, 55], color='gray', linestyle='--', linewidth=1)\n",
    "    ax[i].spines['top'].set_visible(False)\n",
    "    ax[i].spines['right'].set_visible(False)\n",
    "    ax[i].set_xlabel('Year', fontsize=25)\n",
    "    ax[0].set_ylabel('Predict Year', fontsize=25)\n",
    "    ax[i].set_xlim(-5, 50)\n",
    "    ax[i].set_ylim(-5, 50)\n",
    "    ax[i].set_xticks([0, 10, 20, 30, 40], [1500, 1600, 1700, 1800, 1900], rotation=30, fontsize=20)\n",
    "    ax[i].set_yticks([0, 10, 20, 30, 40], [1500, 1600, 1700, 1800, 1900], fontsize=20)\n",
    "    ax[i].grid(True)\n",
    "    ax[i].legend(handles=legend_elements, fontsize=15)\n",
    "\n",
    "ax[0].set_title('5 steps', fontsize=28)\n",
    "ax[1].set_title('10 steps', fontsize=28)\n",
    "ax[2].set_title('20 steps', fontsize=28)\n",
    "ax[3].set_title('30 steps', fontsize=28)\n",
    "\n",
    "plt.savefig('/home/jinny/projects/Art-history/Art-history/graph/supple/figure05_ABCD.png', bbox_inches='tight', transparent=True, dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d185ba2-507b-47c8-ab55-2817d96ec64a",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa775058-6a0f-4b19-aff9-37f4a9051e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_sample = dict()\n",
    "painting_sample[1500] = 31\n",
    "painting_sample[1600] = 31\n",
    "painting_sample[1700] = 70\n",
    "painting_sample[1800] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bba22-eaf4-4d5f-b87b-749952d6b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [1500,1600,1700,1800] : \n",
    "    plt.imshow(plt.imread(base_path+'resized_image/'+painting[year][painting_sample[year]][1]))\n",
    "    print(painting[year][painting_sample[year]][1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bff6538-01d5-4a74-807e-6c634f936ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "for year in [1500,1600,1700,1800] :\n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    gs = GridSpec(2, 5, figure=fig, width_ratios=[2, 1, 1, 1, 1], height_ratios=[1, 1])\n",
    "    \n",
    "    # 첫 번째 큰 그림 (original)\n",
    "    img = plt.imread(base_path+'resized_image/'+painting[year][painting_sample[year]][1])\n",
    "    ax1 = fig.add_subplot(gs[:, 0])  # 첫 번째 열 전체 사용\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')  \n",
    "    # if year == 1500 : ax1.set_title('Original', fontsize=25)\n",
    "    fig.text(0.11, 0.5, f\"{year}\", fontsize=25, va='center', ha='center', rotation=90)\n",
    "\n",
    "    \n",
    "    # 나머지 작은 그림들\n",
    "    i = 0 \n",
    "    for j,step in enumerate(['step5','step10','step20','step30']):\n",
    "        img = using_noprompt['img'][step][year][painting_sample[year]]\n",
    "        ax = fig.add_subplot(gs[i, j + 1])  # 나머지 영역에 채움\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  \n",
    "        # 테두리 추가\n",
    "        rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, linewidth=10, edgecolor=blue_l, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    i = 1\n",
    "    for j,step in enumerate(['step5','step10','step20','step30']):\n",
    "        img = using_prompt['img'][step][year][painting_sample[year]]\n",
    "        ax = fig.add_subplot(gs[i, j + 1])  # 나머지 영역에 채움\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # 테두리 추가\n",
    "        rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, linewidth=10, edgecolor=red_l, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # 간격 조정\n",
    "    gs.update(wspace=0.05, hspace=0.2)\n",
    "    plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/supple/figure05_E_{year}.svg',bbox_inches='tight',transparent = True,dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabba92-bc50-4f96-b697-88981d286547",
   "metadata": {},
   "source": [
    "# Time vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274c587-0a42-4d1e-9096-13630f811f35",
   "metadata": {},
   "source": [
    "#### make time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931be868-c7f5-464b-85af-1c6c202ee409",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec = file_info_latents['cvec'][file_info_latents['new_date'].isin([1900+i*10 for i in range(10)])].values.mean() - file_info_latents['cvec'][file_info_latents['new_date'].isin([1500+i*10 for i in range(10)])].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa42f7c8-2caa-4c5c-96d7-212bbc70693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_time = dict()\n",
    "for year in [1500,1600,1700,1800] :\n",
    "    paths =list()\n",
    "    for path in painting[year] :\n",
    "        paths.append(path[1])\n",
    "    original_time[year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in file_info_latents['cvec'][file_info_latents['Path'].isin(paths)] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a02af5-76cc-47cb-a717-3c938ae076a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using_prompt_time = defaultdict(dict)\n",
    "using_noprompt_time = defaultdict(dict)\n",
    "\n",
    "for step in ['step5','step10','step20','step30'] :\n",
    "    for year in tqdm(using_prompt['cvec'][step].keys()) :\n",
    "        using_prompt_time[step][year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in using_prompt['cvec'][step][year]]\n",
    "\n",
    "for step in ['step5','step10','step20','step30'] :\n",
    "    for year in tqdm(using_noprompt['cvec'][step].keys()) :\n",
    "        using_noprompt_time[step][year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in using_noprompt['cvec'][step][year]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46af48a-6c70-4f83-8889-a5d231f82154",
   "metadata": {},
   "source": [
    "### graph-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74e5a6-6ca8-4561-a2d1-f6ac71934265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import colorsys\n",
    "\n",
    "# 카테고리 설정\n",
    "years = [ 1500, 1600, 1700, 1800 ]\n",
    "names = ['Random\\ndiffusion','Furure\\ndirected']\n",
    "\n",
    "# 팔레트 설정 ('tab20' 팔레트 사용)\n",
    "cmap = plt.get_cmap('Spectral')\n",
    "\n",
    "# 서브플롯 생성\n",
    "for i, year in enumerate(years):\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10,5), sharex=True)\n",
    "    color = cmap(i / len(years))  # 팔레트에서 색상 선택\n",
    "\n",
    "    if i == 0 :\n",
    "    # 색상 채도 조절\n",
    "        rgb = color[:3]\n",
    "        h, s, v = colorsys.rgb_to_hsv(*rgb)\n",
    "        s = s*0.7\n",
    "        v = min(v*1.2,1.0)\n",
    "        new_rgb = colorsys.hsv_to_rgb(h, s, v)\n",
    "        color = (*new_rgb, color[3])\n",
    "\n",
    "    if i == 2 :\n",
    "    # 색상 채도 조절\n",
    "        rgb = color[:3]\n",
    "        h, s, v = colorsys.rgb_to_hsv(*rgb)\n",
    "        h = h*0.85\n",
    "        s = min(s*1.3,1.0)\n",
    "        new_rgb = colorsys.hsv_to_rgb(h, s, v)\n",
    "        color = (*new_rgb, color[3])\n",
    "\n",
    "\n",
    "    # original\n",
    "    axes[0].vlines(np.array(original_time[year]), 0, 1, colors=color, lw=1)\n",
    "    axes[0].vlines(np.median(np.array(original_time[year])), 0, 1, colors='black', alpha=0.7, lw=5)\n",
    "    axes[0].text(-0.92, 0.5, 'Original', verticalalignment='center', fontsize=25)\n",
    "    axes[0].set_xlim(-0.8,-0.2)\n",
    "    axes[0].set_yticks([])\n",
    "    axes[0].tick_params(axis='x', labelsize=20)\n",
    "    #axes[0].set_title('Original', fontsize=20)\n",
    "    \n",
    "    for j, time_dict in enumerate([using_noprompt_time,using_prompt_time]) :\n",
    "        ax = axes[j+1]\n",
    "        try:\n",
    "            temp = time_dict['step20'][year]\n",
    "        except Exception as e:\n",
    "            time_dict['step20'][year] = 0\n",
    "        # 각 카테고리별로 개별 점 그리기\n",
    "        ax.vlines(np.array(time_dict['step10'][year]), 0, 1, colors=color, lw=1)\n",
    "        ax.vlines( np.median(np.array(time_dict['step10'][year])), 0, 1, colors='black', alpha=0.7, lw=5)\n",
    "    \n",
    "        # 카테고리 이름 추가\n",
    "        ax.text(-0.92, 0.5, names[j], verticalalignment='center', fontsize=25)\n",
    "        ax.set_xlim(-0.8,-0.2)\n",
    "        ax.set_yticks([])\n",
    "        ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "    # 테두리 설정\n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_edgecolor('black')  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "\n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_edgecolor(blue_l)  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "    \n",
    "    for spine in axes[2].spines.values():\n",
    "        spine.set_edgecolor(red_l)  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/supple/figure05_F_{year}.svg',bbox_inches='tight',transparent = True,dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeabead-a435-4621-986e-d4fa2e2ca38a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Diffusion Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d57dad-9ad7-48c0-a7d2-983d69c22b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4075fd-175f-47e9-8551-cbecf128a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_a1 = list()\n",
    "for year in painting_200_a.keys() :\n",
    "    for i in painting_200_a[year] :\n",
    "        temp_a1.append(i)\n",
    "\n",
    "temp_a2 = list()\n",
    "for year in painting_200_c.keys() :\n",
    "    for i in painting_200_c[year] :\n",
    "        temp_a2.append(i.reshape(-1))\n",
    "\n",
    "## all vector\n",
    "n_chunks = 100\n",
    "def chunked_pdist(data_chunk):\n",
    "    return pdist(data_chunk, metric='euclidean')\n",
    "\n",
    "dist_original = dict()\n",
    "temp = np.array_split(np.array(temp_a1), n_chunks)\n",
    "dist_original['avec'] = np.hstack(Parallel(n_jobs=-1)(delayed(chunked_pdist)(chunk) for chunk in temp))\n",
    "\n",
    "temp = np.array_split(np.array(temp_a2), n_chunks)\n",
    "dist_original['cvec'] = np.hstack(Parallel(n_jobs=-1)(delayed(chunked_pdist)(chunk) for chunk in temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d24f61-74ff-4b21-9017-9abdf896e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_r_noprompt = defaultdict(dict)\n",
    "dist_pns_nextyear = defaultdict(dict)\n",
    "\n",
    "for step in tqdm(['step5','step10','step20','step30']) :\n",
    "\n",
    "    temp_b = list()\n",
    "    for year in r_noprompt['avec'][step].keys() :\n",
    "        for i in r_noprompt['avec'][step][year] :\n",
    "            temp_b.append(np.array(i.reshape(-1).cpu()))\n",
    "    dist_r_noprompt['avec'][step] = euclidean_distances(temp_a1,temp_b).reshape(-1)\n",
    "\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    temp_b = list()\n",
    "    for year in pns_nextyear['avec'][step].keys() :\n",
    "        for i in pns_nextyear['avec'][step][year] :\n",
    "            temp_b.append(np.array(i.reshape(-1).cpu()))\n",
    "    dist_pns_nextyear['avec'][step] = euclidean_distances(temp_a1,temp_b).reshape(-1)\n",
    "\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    temp_b = list()\n",
    "    for year in r_noprompt['cvec'][step].keys() :\n",
    "        for i in r_noprompt['cvec'][step][year] :\n",
    "            temp_b.append(np.array(i.reshape(-1).cpu()))\n",
    "    dist_r_noprompt['cvec'][step] = euclidean_distances(temp_a2,temp_b).reshape(-1)\n",
    "\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    temp_b = list()\n",
    "    for year in pns_nextyear['cvec'][step].keys() :\n",
    "        for i in pns_nextyear['cvec'][step][year] :\n",
    "            temp_b.append(np.array(i.reshape(-1).cpu()))\n",
    "    dist_pns_nextyear['cvec'][step] = euclidean_distances(temp_a2,temp_b).reshape(-1)\n",
    "\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce768ef-9a08-4621-beef-06b17de81298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for vec in ['avec','cvec'] :\n",
    "\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    #sns.histplot(dist_original[vec], alpha=0.4, element='step')    \n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        sns.distplot(dist_pns_nextyear[vec][step], hist=False)   \n",
    "    ax.set_title(\"A-vector Artist Distribution\",size=30 )\n",
    "    ax.set_xlabel(\"Euclidean distance\",size=22)\n",
    "    ax.set_ylabel(\"Counts\",size=22)\n",
    "    #ax.set_xlim(50,310)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(),size=22)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),size=22)\n",
    "    \n",
    "    #plt.savefig(f'/home/jinny/projects/Art-history/make_graph/graph/supple/si07_a1.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42744cf7-a833-4f1b-baad-ca8f036109e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for vec in ['avec','cvec'] :\n",
    "\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    #sns.histplot(dist_original[vec], alpha=0.4, element='step')    \n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        sns.distplot(dist_r_noprompt[vec][step], hist=False)\n",
    "    ax.set_title(\"A-vector Artist Distribution\",size=30 )\n",
    "    ax.set_xlabel(\"Euclidean distance\",size=22)\n",
    "    ax.set_ylabel(\"Counts\",size=22)\n",
    "    #ax.set_xlim(50,310)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(),size=22)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),size=22)\n",
    "    \n",
    "    #plt.savefig(f'/home/jinny/projects/Art-history/make_graph/graph/supple/si07_a1.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efee1da-bb85-4daf-9e25-1350ca02f926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "history",
   "language": "python",
   "name": "history"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
