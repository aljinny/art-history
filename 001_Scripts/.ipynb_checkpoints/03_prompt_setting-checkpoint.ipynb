{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6146e6a-6f39-4351-96e3-5a7514420571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.font_manager as fm\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import unicodedata\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48426ae8-ac09-4388-bf43-0676396d5678",
   "metadata": {},
   "source": [
    "## Prompt Setting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fe33aef-e0f9-42df-be90-6d72e365338f",
   "metadata": {},
   "source": [
    "path = '/home/jinny/projects/Art-history/Art-history/datas/'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71f6e888-6069-449e-99d3-f177ea944567",
   "metadata": {},
   "source": [
    "prompts_00 = np.load( path+'vectors/prompts_00.npy', allow_pickle=True)\n",
    "prompts_01 = np.load( path+'vectors/prompts_01.npy', allow_pickle=True)\n",
    "prompts_02 = np.load( path+'vectors/prompts_02.npy', allow_pickle=True)\n",
    "prompts_03 = np.load( path+'vectors/prompts_03.npy', allow_pickle=True)\n",
    "prompts_04 = np.load( path+'vectors/prompts_04.npy', allow_pickle=True)\n",
    "prompts_05 = np.load( path+'vectors/prompts_05.npy', allow_pickle=True)\n",
    "\n",
    "prompts = pd.concat([pd.DataFrame(prompts_00,columns=['prompts','Path']), \n",
    "                     pd.DataFrame(prompts_01,columns=['prompts','Path']), \n",
    "                     pd.DataFrame(prompts_02,columns=['prompts','Path']),\n",
    "                     pd.DataFrame(prompts_03,columns=['prompts','Path']),\n",
    "                     pd.DataFrame(prompts_04,columns=['prompts','Path']),\n",
    "                     pd.DataFrame(prompts_05,columns=['prompts','Path'])]) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "83b128cb-9c53-4c41-8a74-c5b786ca8ca3",
   "metadata": {},
   "source": [
    "#### Prompt Preprocessing ( stopword & axcii )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d902761-9cec-4f7c-8ebd-f2d6eae4ca2e",
   "metadata": {},
   "source": [
    "# Stop word setting\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('arafed')\n",
    "stop_words = set(stop_words) \n",
    "\n",
    "# AXCII filter\n",
    "def axcii_del(input_text):\n",
    "    normalized = unicodedata.normalize('NFD', input_text)\n",
    "    return ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4b86a09-d873-43a9-9cce-798d2dfa17cd",
   "metadata": {},
   "source": [
    "temp = list()\n",
    "for i in tqdm(prompts['prompts']) :\n",
    "    \n",
    "    word_tokens = word_tokenize(i)\n",
    "\n",
    "    # stop word & axcii & \n",
    "    result = list()\n",
    "    for word in word_tokens: \n",
    "        if word not in stop_words:\n",
    "            result.append(''.join([re.sub(r\"[^a-z0-9]\", \" \", i.lower()) for i in axcii_del(word)]))\n",
    "    result = ' '.join(result)\n",
    "    temp.append(result)\n",
    "\n",
    "temp = [re.sub(r\"\\s+\", \" \", i) for i in temp]\n",
    "prompts['new_prompts'] = temp"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f251de32-d598-42f3-bd31-4f5a30525b88",
   "metadata": {},
   "source": [
    "#### save"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22565182-1f53-44f1-983d-a40c78a9cbd4",
   "metadata": {},
   "source": [
    "prompts.to_csv(path+'prompts.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca049e-e9b3-444a-90d6-72655a38bd72",
   "metadata": {},
   "source": [
    "## Make Word Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "54a14143-f261-4d1c-a2be-981621825894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "397af2a6-20ca-4a04-94d3-b576cd64ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.read_csv(path+'prompts.csv')\n",
    "file_info = pd.read_csv(path+'file_info.csv')\n",
    "file_info_prompts = pd.merge(file_info[['Path','new_date']], prompts[['Path','new_prompts']], how = 'left', on = 'Path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e7a5b5eb-83ab-48aa-8df3-2d38879ba0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = list(file_info_prompts['new_prompts'][file_info_prompts['new_date']>=1500])\n",
    "all_countv = CountVectorizer(binary=True).fit(prompt)\n",
    "all_countv_matrix = all_countv.transform(prompt).toarray()\n",
    "all_countv_voca = all_countv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "7300922e-1f11-46e1-bf90-f01f09ec3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voca = pd.DataFrame(all_countv_voca.items(),columns=['word','index'])\n",
    "df_voca = df_voca.sort_values('index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e304918f-8f67-4bd6-8797-79e34fc4986c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.00it/s]\n"
     ]
    }
   ],
   "source": [
    "years = list(range(1500, 2000, 10))\n",
    "\n",
    "all_countv_freq = np.sum(all_countv_matrix, axis=0)\n",
    "all_countv_voca_rev = {value: key for key, value in all_countv_voca.items()}\n",
    "df_count_by_year = pd.DataFrame([[0]*len(years)]*len(all_countv_voca_rev),columns=years)\n",
    "\n",
    "for year in tqdm(years) :\n",
    "    prompt = file_info_prompts['new_prompts'][file_info_prompts['new_date']==year]\n",
    "    countv = CountVectorizer(binary=True).fit(prompt)\n",
    "    countv_freq = np.sum(countv.transform(prompt).toarray(), axis=0)\n",
    "\n",
    "    temp = [0]*len(all_countv_voca_rev)\n",
    "    for x in countv.vocabulary_.items() :\n",
    "        temp[all_countv_voca[x[0]]]=countv_freq[x[1]]\n",
    "    df_count_by_year[year] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7598309c-15ed-4e1f-b5b7-ef1aec04ef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words : 21575\n"
     ]
    }
   ],
   "source": [
    "print(f'total words : {len(df_voca)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "07ec6993-d6b0-4833-9060-d0c912c409d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voca.to_csv(path+'words/vocas.csv',index=False)\n",
    "df_count_by_year.to_csv(path+'words/vocas_counting.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d1cd4-0782-4a8b-acf5-7c0142541f0d",
   "metadata": {},
   "source": [
    "## Make Word Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee1457-c507-4917-bf98-3b03222307a1",
   "metadata": {},
   "source": [
    "#### Prompt preprocessing ( style & artist del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5b157dc4-f02c-4187-9fe0-bb023e06ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artist keyword\n",
    "with open(path+'blip_artists.txt', \"r\") as tf:\n",
    "    blip_artists = tf.read().split(\"\\n\")\n",
    "blip_artists = [re.sub(r\"[-,.,',\\s+]\", \" \", i.lower()) for i in blip_artists]\n",
    "artists = list()\n",
    "for artist in blip_artists :\n",
    "    artists.extend(axcii_del(artist).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fa62c1f4-bd39-470f-a98e-f0e9c09636ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = file_info['Style'].dropna().unique()\n",
    "styles = [re.sub(r\"\\s+\", \" \", i.lower()) for i in styles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d323d8d4-b43e-4a94-9796-2f0d9781ef92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 73814/73814 [10:48<00:00, 113.91it/s]\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for text in tqdm(file_info_prompts['new_prompts']):\n",
    "\n",
    "    # style del\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, styles)) + r')\\b'\n",
    "    result = re.sub(pattern, '', text)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    \n",
    "    # blip artist del\n",
    "    pattern = r'\\b(' + '|'.join(map(re.escape, artists)) + r')\\b'\n",
    "    result = re.sub(pattern, '', result)\n",
    "    result = re.sub(r'\\s+', ' ', result).strip()\n",
    "    results.append(result)\n",
    "    \n",
    "file_info_prompts['new_prompt_tokenver'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da883266-0011-4aeb-8575-b35fbfe4fa13",
   "metadata": {},
   "source": [
    "#### tf-idf and make token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "cf20b2d3-586b-4ff5-80b7-6c59d0363c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "prompt = list(file_info_prompts['new_prompt_tokenver'][file_info_prompts['new_date']>=1500])\n",
    "tfidfv = TfidfVectorizer(stop_words=None, min_df=1, max_df=1.0).fit(prompt)\n",
    "tfidfv_matrix = tfidfv.transform(prompt).toarray()\n",
    "tfidfv_voca = tfidfv.vocabulary_\n",
    "tfidfv_voca_rev = {value: key for key, value in tfidfv_voca.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b613e5c4-e891-4981-bc72-6a5dca40bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = tfidfv.idf_\n",
    "idf_dict = dict(zip(tfidfv.get_feature_names_out(), idf_values))\n",
    "df_tfidfv_matrix = pd.DataFrame(tfidfv_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "798bffe9-0c44-4284-b2b4-05d92e8f095f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### ver01 - select decade\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tokens_temp = defaultdict(list)\n",
    "for col in tqdm(df_tfidfv_matrix.columns) : \n",
    "    tokens_temp[int(file_info_prompts.loc[df_tfidfv_matrix[col].idxmax()]['new_date']/100)*100].append(col)\n",
    "\n",
    "tokens = dict()\n",
    "for year in [1500+i*100 for i in range(5)] :\n",
    "    years = [year+j*10 for j in range(10)]\n",
    "    \n",
    "    idxs = file_info_prompts[file_info_prompts['new_date'].isin(years)].index\n",
    "    temp = df_tfidfv_matrix.iloc[idxs]\n",
    "\n",
    "    temp = temp[tokens_temp[year]].sum().nlargest(77)\n",
    "    tokens[year] = [tfidfv_voca_rev[k] for k in temp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "866933f3-296f-4714-a317-19b51d4d41dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ver02 - select century\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "tfidf100 = dict()\n",
    "for year in [1500+i*100 for i in range(5)] :\n",
    "    years = [year+j*10 for j in range(10)]\n",
    "    idxs = file_info_prompts[file_info_prompts['new_date'].isin(years)].index\n",
    "    tfidf100[year] = df_tfidfv_matrix[df_tfidfv_matrix.index.isin(idxs)].sum()\n",
    "\n",
    "df_tfidf100 = pd.DataFrame(tfidf100)\n",
    "df_tfidf100 = df_tfidf100[(df_tfidf100[1500]!=0) | (df_tfidf100[1600]!=0) | (df_tfidf100[1700]!=0) | (df_tfidf100[1800]!=0) | (df_tfidf100[1900]!=0)]\n",
    "\n",
    "tokens_temp = defaultdict(list)\n",
    "for i in df_tfidf100.index :\n",
    "    year = df_tfidf100.loc[i].idxmax()\n",
    "    tfidf = df_tfidf100.loc[i][year]\n",
    "    tokens_temp[year].append([i,tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "20e4fc1b-cf69-42a3-9d7e-1a87f081b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dict()\n",
    "for year in [1600+i*100 for i in range(4)] :\n",
    "    temp = pd.DataFrame(tokens_temp[year],columns=['word_idx','tfidf'])\n",
    "    temp['word'] = [ tfidfv_voca_rev[idx] for idx in temp['word_idx'] ]\n",
    "    temp = temp.sort_values('tfidf',ascending=False)[:100]\n",
    "    tokens[year] = temp['word'].values\n",
    "    tokens[f'{year}_tfidf'] = temp['tfidf'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "8ae5398f-248b-4a0b-8667-34cb967752d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tokens).to_csv(path+f'words/tokens_100.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0716eae8-4da2-4fa1-bf1b-fea20c358fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_names = [\n",
    "    'lieven',  \n",
    "    'vemeer',  \n",
    "    'carvaggio', \n",
    "    'carravaggio',\n",
    "    'wissing',\n",
    "    'neoclassicist',\n",
    "    'barocco',\n",
    "    'rokoko', \n",
    "    'portraiit',\n",
    "    'pitt',\n",
    "    'abstract',\n",
    "    'portrait',\n",
    "    'landscape',\n",
    "    'todorovitch',\n",
    "    '1647',\n",
    "    '1666',\n",
    "    '1600s',\n",
    "    '1614572159',\n",
    "    '1615',\n",
    "    '16384k',\n",
    "    '1759',\n",
    "    '1786560639',\n",
    "    '2k',\n",
    "    '3000',\n",
    "    '40s',\n",
    "    '640',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "4ee95404-e740-4bda-aff4-88762d0f9d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tokens = pd.DataFrame(tokens)\n",
    "tokens = dict()\n",
    "for year in [1600,1700,1800,1900] :\n",
    "    temp = df_tokens[~df_tokens[year].isin(artist_names)][:77]\n",
    "    tokens[year] = temp[year].values\n",
    "    tokens[f'{year}_tfidf'] = temp[f'{year}_tfidf'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "6478de04-68b5-44c8-99cf-345d6fb7cafd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(path+'words/tokens.pkl', 'wb') as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "a0ed3889-31c3-4e0b-98cd-9dd8aafa5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tokens).to_csv(path+f'words/tokens.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70f34b-b411-477a-a582-46a439f36f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "history",
   "language": "python",
   "name": "history"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
