{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8c64b0-5873-4291-b618-06a42b08dbdc",
   "metadata": {},
   "source": [
    "#### vector import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8805334-3da5-4014-a176-cec3e960f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import umap\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from PIL import Image,ImageOps\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import torch\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5366f-c529-426d-8ee0-01c0a17e4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/jinny/projects/Art-history/Art-history/datas/'\n",
    "file_info = pd.read_csv(base_path+'file_info.csv')\n",
    "\n",
    "# file_info_latent\n",
    "df = pd.DataFrame(( np.load(base_path+'vectors/avec_latents.npy', allow_pickle=True)),columns=['avec','Path'])\n",
    "file_info_latents = pd.merge(file_info, df, how = 'left', on = 'Path')\n",
    "file_info_latents = file_info_latents[~file_info_latents.avec.isnull()]\n",
    "\n",
    "avec = np.array([i.reshape(-1) for i in file_info_latents['avec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926ea1b-72b8-46b3-95ba-8177cc49ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.load(base_path+'vectors/cvec_latents.npy', allow_pickle=True),columns=['cvec','Path'])\n",
    "file_info_latents = pd.merge(file_info_latents, df, how = 'left', on = 'Path')\n",
    "\n",
    "cvec = np.array([i.reshape(-1) for i in file_info_latents['cvec']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6133c66-ae06-458c-9107-4b808f7c8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'words/tokens.pkl', 'rb') as file:\n",
    "    tokens = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d721635-aa07-473e-a371-8d9566487b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path+'vectors/diffusion/diffusion_sample.pkl', 'rb') as file:\n",
    "    painting = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f26795-e927-4b75-9196-4df1a8d7f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_prompt = defaultdict(dict)\n",
    "using_noprompt = defaultdict(dict)\n",
    "\n",
    "for vec in ['avec','cvec','img'] :\n",
    "    vec_path = f'vectors/diffusion/no_comma/{vec}/'\n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        with open(base_path + vec_path + step+'/using_prompt.pkl', 'rb') as file:\n",
    "            using_prompt[vec][step] = pickle.load(file)\n",
    "\n",
    "for vec in ['avec','cvec','img'] :\n",
    "    vec_path = f'vectors/diffusion/no_comma/{vec}/'\n",
    "    for step in ['step5','step10','step20','step30'] :\n",
    "        with open(base_path + vec_path + step+'/using_noprompt.pkl', 'rb') as file:\n",
    "            using_noprompt[vec][step] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103399-c8e2-48bd-bd47-9f187799f278",
   "metadata": {},
   "source": [
    "#### diffusion import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a5224-eefe-46f9-91f2-3a04e461735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceee339-53bb-4c9f-965e-3c8be6421249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "from contextlib import nullcontext\n",
    "from pytorch_lightning import seed_everything\n",
    "from imwatermark import WatermarkEncoder\n",
    "import pandas as pd\n",
    "from scripts.txt2img import put_watermark\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b0060-8016-4c50-8efe-9c7c89966598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model(device_num) :\n",
    "    base_path = '/home/jinny/projects/Art-history/Art-history/'\n",
    "    config = OmegaConf.load(base_path+f\"scripts/configs/stable-diffusion/v2-inference.yaml\")\n",
    "    model = load_model_from_config(config, base_path+f\"scripts/configs/checkpoint/512-base-ema.ckpt\") \n",
    "\n",
    "    GPU_NUM = device_num # 원하는 GPU 번호 입력\n",
    "    \n",
    "    # GPU 할당 변경하기\n",
    "    device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.set_device(device) # change allocation of current GPU\n",
    "    print ('Current cuda device ', torch.cuda.current_device()) # check\n",
    "    \n",
    "    model = model.to(device)\n",
    "    return model,device\n",
    "\n",
    "def load_img(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2. * image - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554be3f-2ae6-455b-b6d2-b82f267e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2latent(path,model) :\n",
    "    file_info = pd.read_csv(base_path+'file_info.csv')\n",
    "\n",
    "    image_latent = list()\n",
    "    for i in path :\n",
    "        init_image = load_img(base_path+'resized_image/'+i).to(device)\n",
    "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "        init_latent = init_latent.view([-1])\n",
    "        image_latent.append(init_latent.cpu().numpy())\n",
    "\n",
    "    #latent_df = pd.DataFrame(image_latent)\n",
    "    return image_latent\n",
    "\n",
    "def model_2img(latent_samples, model) :\n",
    "    precision_scope = autocast \n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                \n",
    "                x_samples = model.decode_first_stage(latent_samples)\n",
    "                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                for x_sample in x_samples:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                    #img = put_watermark(img, wm_encoder)\n",
    "                    return img\n",
    "                    # plt.imshow(img)\n",
    "                    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb33c61-9cb8-4789-9ec7-c388b6b3090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diffusion_model_changing_denoising_steps(init_latent,prompt,n,num,batch,model,device,step):  \n",
    "\n",
    "    seed_everything(42)\n",
    "    \n",
    "    ddim_steps = 50\n",
    "    ddim_eta = 0.0\n",
    "    n_iter = 1\n",
    "    batch_size = batch\n",
    "    scale = 9.0\n",
    "    \n",
    "    sampler = DDIMSampler(model)\n",
    "    sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "    precision_scope = autocast\n",
    "    \n",
    "    all_samples = []\n",
    "    all_samples_enc = []\n",
    "    for i in range(n):\n",
    "        #strength = 0.8\n",
    "        strength = 1/ddim_steps * (i+num) # change strength to control denoising steps\n",
    "        data = [batch_size * [prompt]]\n",
    "        t_enc = step\n",
    "        sampler.make_schedule(ddim_num_steps=ddim_steps, ddim_eta=ddim_eta, verbose=False)\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                        for prompts in tqdm(data, desc=\"data\"):\n",
    "                            uc = None\n",
    "                            if scale != 1.0:\n",
    "                                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                            if isinstance(prompts, tuple):\n",
    "                                prompts = list(prompts)\n",
    "                            c = model.get_learned_conditioning(prompts)\n",
    "                            if prompt == \"no prompt\" :\n",
    "                                print(\"change prompt vector to zero\")\n",
    "                                c = torch.zeros(c.shape[0], c.shape[1], c.shape[2]).to('cuda:1') # change prompt vector to zero (i.e., no direction)\n",
    "\n",
    "                            # encode (scaled latent)\n",
    "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc] * batch_size).to(device))\n",
    "                            # decode it\n",
    "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
    "                                                     unconditional_conditioning=uc, )\n",
    "\n",
    "                            x_samples = model.decode_first_stage(samples)\n",
    "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                            \n",
    "        #draw_image(x_samples[0])\n",
    "        all_samples.append(x_samples)\n",
    "        all_samples_enc.append(samples)\n",
    "               \n",
    "    return all_samples_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfe5c6-6f1e-49e1-8366-15010a698473",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model load\n",
    "model,device = load_model(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769636-cd53-404e-99a4-ee08127ccf50",
   "metadata": {},
   "source": [
    "# regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851cc9d6-3d1c-41b6-aee6-b554fa9f61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8e53a-76fb-4c59-9a6a-7ec87deaff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "cvec_xgb_model = xgb.XGBRegressor()\n",
    "cvec_xgb_model.load_model(base_path+\"models/cvec_xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1a50c5-fecc-4c93-aa36-4c791e9488e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ori = list()\n",
    "for year in [1500, 1600, 1700, 1800] :\n",
    "    y_ori.extend([int((file_info_latents['new_date'][file_info_latents['Path']==i[1]].values[0]%1500)/10) for i in painting[year]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a920f73-db39-4452-aaa6-3b9f0c8e840a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pre_diff = dict()\n",
    "\n",
    "y_pre_diff[5] = defaultdict(list)\n",
    "y_pre_diff[10] = defaultdict(list)\n",
    "y_pre_diff[20] = defaultdict(list)\n",
    "y_pre_diff[30] = defaultdict(list)\n",
    "\n",
    "for year in [1500, 1600, 1700, 1800] :\n",
    "    for step in [5,10,20,30] :\n",
    "        y_pre_diff[step]['u_prompt'].extend(cvec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1).cpu()) for i in using_prompt['cvec'][f'step{step}'][year]]))))\n",
    "        y_pre_diff[step]['u_noprompt'].extend(cvec_xgb_model.predict(pd.DataFrame(np.array([np.array(i.reshape(-1).cpu()) for i in using_noprompt['cvec'][f'step{step}'][year]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d39d05-2f0d-475a-a5b3-709142ff14c4",
   "metadata": {},
   "source": [
    "### graph-abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c1a6c-572f-42fd-a1ae-f4f3f50b0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(25, 5))\n",
    "\n",
    "blue_l = '#0054FF'\n",
    "blue_s = '#78ACF6'\n",
    "red_l = '#FF4848'\n",
    "red_s = '#FFA2A2'\n",
    "\n",
    "for idx, step in enumerate([5, 10, 20, 30]):\n",
    "    sns.regplot(\n",
    "        x=y_ori,\n",
    "        y=y_pre_diff[step]['u_noprompt'],\n",
    "        scatter_kws={'s': 10, 'alpha': 0.3, 'color': blue_s},\n",
    "        line_kws={'color': blue_l},\n",
    "        ax=ax[idx],\n",
    "        lowess=True,\n",
    "    )\n",
    "    sns.regplot(\n",
    "        x=y_ori,\n",
    "        y=y_pre_diff[step]['u_prompt'],\n",
    "        scatter_kws={'s': 10, 'alpha': 0.3, 'color': red_s},\n",
    "        line_kws={'color': red_l},\n",
    "        ax=ax[idx],\n",
    "        lowess=True,\n",
    "    )\n",
    "\n",
    "# 범례 라인 생성\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=blue_l, lw=2, label='Random diffusion'),\n",
    "    Line2D([0], [0], color=red_l, lw=2, label='Future directed'),\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].plot([-5, 55], [-5, 55], color='gray', linestyle='--', linewidth=1)\n",
    "    ax[i].spines['top'].set_visible(False)\n",
    "    ax[i].spines['right'].set_visible(False)\n",
    "    ax[i].set_xlabel('Year', fontsize=25)\n",
    "    ax[0].set_ylabel('Predict Year', fontsize=25)\n",
    "    ax[i].set_xlim(-5, 50)\n",
    "    ax[i].set_ylim(-5, 50)\n",
    "    ax[i].set_xticks([0, 10, 20, 30, 40], [1500, 1600, 1700, 1800, 1900], rotation=30, fontsize=20)\n",
    "    ax[i].set_yticks([0, 10, 20, 30, 40], [1500, 1600, 1700, 1800, 1900], fontsize=20)\n",
    "    ax[i].grid(True)\n",
    "    ax[i].legend(handles=legend_elements, fontsize=15)\n",
    "\n",
    "ax[0].set_title('5 steps', fontsize=28)\n",
    "ax[1].set_title('10 steps', fontsize=28)\n",
    "ax[2].set_title('20 steps', fontsize=28)\n",
    "ax[3].set_title('30 steps', fontsize=28)\n",
    "\n",
    "plt.savefig('/home/jinny/projects/Art-history/Art-history/graph/figure05_ABCD.svg', bbox_inches='tight', transparent=True, dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420b1ee2-4faa-41eb-9d89-776fc59b2e1b",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d2f23-777f-4a12-9580-aa27d6519e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "painting_sample = dict()\n",
    "painting_sample[1500] = 31\n",
    "painting_sample[1600] = 31\n",
    "painting_sample[1700] = 70\n",
    "painting_sample[1800] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560f43c-e2f9-4bb5-a660-78a5b6d5c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [1500,1600,1700,1800] : \n",
    "    plt.imshow(plt.imread(base_path+'resized_image/'+painting[year][painting_sample[year]][1]))\n",
    "    print(painting[year][painting_sample[year]][1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c31f5-268f-481f-9ba9-ca83722969e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "for year in [1500,1600,1700,1800] :\n",
    "    fig = plt.figure(figsize=(13, 4))\n",
    "    gs = GridSpec(2, 5, figure=fig, width_ratios=[2, 1, 1, 1, 1], height_ratios=[1, 1])\n",
    "    \n",
    "    # 첫 번째 큰 그림 (original)\n",
    "    img = plt.imread(base_path+'resized_image/'+painting[year][painting_sample[year]][1])\n",
    "    ax1 = fig.add_subplot(gs[:, 0])  # 첫 번째 열 전체 사용\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')  \n",
    "    # if year == 1500 : ax1.set_title('Original', fontsize=25)\n",
    "    fig.text(0.11, 0.5, f\"{year}\", fontsize=25, va='center', ha='center', rotation=90)\n",
    "\n",
    "    \n",
    "    # 나머지 작은 그림들\n",
    "    i = 0 \n",
    "    for j,step in enumerate(['step5','step10','step20','step30']):\n",
    "        img = using_noprompt['img'][step][year][painting_sample[year]]\n",
    "        ax = fig.add_subplot(gs[i, j + 1])  # 나머지 영역에 채움\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  \n",
    "        # 테두리 추가\n",
    "        rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, linewidth=10, edgecolor=blue_l, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    i = 1\n",
    "    for j,step in enumerate(['step5','step10','step20','step30']):\n",
    "        img = using_prompt['img'][step][year][painting_sample[year]]\n",
    "        ax = fig.add_subplot(gs[i, j + 1])  # 나머지 영역에 채움\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        # 테두리 추가\n",
    "        rect = Rectangle((0, 0), 1, 1, transform=ax.transAxes, linewidth=10, edgecolor=red_l, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # 간격 조정\n",
    "    gs.update(wspace=0.05, hspace=0.2)\n",
    "    plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/figure05_E_{year}.svg',bbox_inches='tight',transparent = True,dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4681b51-e1e0-4b8b-90c4-77cd26e761f8",
   "metadata": {},
   "source": [
    "# Time vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8118429-1d7e-49c7-84d4-d8dff5fce4a7",
   "metadata": {},
   "source": [
    "#### make time vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb7dff-e2a4-4d87-81c9-cfc19c37e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec = file_info_latents['cvec'][file_info_latents['new_date'].isin([1900+i*10 for i in range(10)])].values.mean() - file_info_latents['cvec'][file_info_latents['new_date'].isin([1500+i*10 for i in range(10)])].values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e5b92-7883-4a02-9bac-012be73da728",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_time = dict()\n",
    "for year in [1500,1600,1700,1800] :\n",
    "    paths =list()\n",
    "    for path in painting[year] :\n",
    "        paths.append(path[1])\n",
    "    original_time[year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in file_info_latents['cvec'][file_info_latents['Path'].isin(paths)] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cd186-b9d8-4972-8748-16bdab7598f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using_prompt_time = defaultdict(dict)\n",
    "using_noprompt_time = defaultdict(dict)\n",
    "\n",
    "for step in ['step5','step10','step20','step30'] :\n",
    "    for year in tqdm(using_prompt['cvec'][step].keys()) :\n",
    "        using_prompt_time[step][year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in using_prompt['cvec'][step][year]]\n",
    "\n",
    "for step in ['step5','step10','step20','step30'] :\n",
    "    for year in tqdm(using_noprompt['cvec'][step].keys()) :\n",
    "        using_noprompt_time[step][year] = [ np.dot(time_vec, i[0]) / np.linalg.norm(time_vec) for i in using_noprompt['cvec'][step][year]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78c900-b20b-4f7d-bd2c-233fe27cf249",
   "metadata": {},
   "source": [
    "### graph-f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a74a4c-831d-4e8a-b532-ad847af175d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import colorsys\n",
    "\n",
    "# 카테고리 설정\n",
    "years = [ 1500, 1600, 1700, 1800 ]\n",
    "names = ['Random\\ndiffusion','Furure\\ndirected']\n",
    "\n",
    "# 팔레트 설정 ('tab20' 팔레트 사용)\n",
    "cmap = plt.get_cmap('Spectral')\n",
    "\n",
    "# 서브플롯 생성\n",
    "for i, year in enumerate(years):\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10,5), sharex=True)\n",
    "    color = cmap(i / len(years))  # 팔레트에서 색상 선택\n",
    "\n",
    "    if i == 0 :\n",
    "    # 색상 채도 조절\n",
    "        rgb = color[:3]\n",
    "        h, s, v = colorsys.rgb_to_hsv(*rgb)\n",
    "        s = s*0.7\n",
    "        v = min(v*1.2,1.0)\n",
    "        new_rgb = colorsys.hsv_to_rgb(h, s, v)\n",
    "        color = (*new_rgb, color[3])\n",
    "\n",
    "    if i == 2 :\n",
    "    # 색상 채도 조절\n",
    "        rgb = color[:3]\n",
    "        h, s, v = colorsys.rgb_to_hsv(*rgb)\n",
    "        h = h*0.85\n",
    "        s = min(s*1.3,1.0)\n",
    "        new_rgb = colorsys.hsv_to_rgb(h, s, v)\n",
    "        color = (*new_rgb, color[3])\n",
    "\n",
    "\n",
    "    # original\n",
    "    axes[0].vlines(np.array(original_time[year]), 0, 1, colors=color, lw=1)\n",
    "    axes[0].vlines(np.median(np.array(original_time[year])), 0, 1, colors='black', alpha=0.7, lw=5)\n",
    "    axes[0].text(-0.92, 0.5, 'Original', verticalalignment='center', fontsize=25)\n",
    "    axes[0].set_xlim(-0.8,-0.2)\n",
    "    axes[0].set_yticks([])\n",
    "    axes[0].tick_params(axis='x', labelsize=20)\n",
    "    #axes[0].set_title('Original', fontsize=20)\n",
    "    \n",
    "    for j, time_dict in enumerate([using_noprompt_time,using_prompt_time]) :\n",
    "        ax = axes[j+1]\n",
    "        try:\n",
    "            temp = time_dict['step20'][year]\n",
    "        except Exception as e:\n",
    "            time_dict['step20'][year] = 0\n",
    "        # 각 카테고리별로 개별 점 그리기\n",
    "        ax.vlines(np.array(time_dict['step10'][year]), 0, 1, colors=color, lw=1)\n",
    "        ax.vlines( np.median(np.array(time_dict['step10'][year])), 0, 1, colors='black', alpha=0.7, lw=5)\n",
    "    \n",
    "        # 카테고리 이름 추가\n",
    "        ax.text(-0.92, 0.5, names[j], verticalalignment='center', fontsize=25)\n",
    "        ax.set_xlim(-0.8,-0.2)\n",
    "        ax.set_yticks([])\n",
    "        ax.tick_params(axis='x', labelsize=20)\n",
    "\n",
    "    # 테두리 설정\n",
    "    for spine in axes[0].spines.values():\n",
    "        spine.set_edgecolor('black')  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "\n",
    "    for spine in axes[1].spines.values():\n",
    "        spine.set_edgecolor(blue_l)  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "    \n",
    "    for spine in axes[2].spines.values():\n",
    "        spine.set_edgecolor(red_l)  # 테두리 색상 변경\n",
    "        spine.set_linewidth(5)  # 테두리 두께 설정\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'/home/jinny/projects/Art-history/Art-history/graph/figure05_F_{year}.svg',bbox_inches='tight',transparent = True,dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e9b95-f29f-404f-8599-06ac96f11d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35c799-91eb-4da5-bc08-56e3035ffa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "history",
   "language": "python",
   "name": "history"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
